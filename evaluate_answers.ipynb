{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8462e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b47f64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_answer\u001b[39m(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     instruction: \u001b[38;5;28mstr\u001b[39m, answer: \u001b[38;5;28mstr\u001b[39m, client: \u001b[43mOpenAI\u001b[49m\n\u001b[32m      3\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m      4\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an expert judge. Please evaluate the quality of a given answer to an instruction based on two criteria: \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33m        1. Accuracy: How factually correct is the information presented in the answer? You are a technical expert in this topic.\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m        2. Style: Is the tone and writing style appropriate for a blog post or social media content? It should use simple but technical words and avoid formal or complex language.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m \u001b[33m        \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     38\u001b[39m     completion = client.chat.completions.create(\n\u001b[32m     39\u001b[39m             model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m             messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m             temperature=\u001b[32m0.8\u001b[39m\n\u001b[32m     55\u001b[39m         )\n",
      "\u001b[31mNameError\u001b[39m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_answer(\n",
    "    instruction: str, answer: str, client: OpenAI\n",
    ") -> dict:\n",
    "    prompt = f\"\"\"You are an expert judge. Please evaluate the quality of a given answer to an instruction based on two criteria: \\\n",
    "        1. Accuracy: How factually correct is the information presented in the answer? You are a technical expert in this topic.\n",
    "        2. Style: Is the tone and writing style appropriate for a blog post or social media content? It should use simple but technical words and avoid formal or complex language.\n",
    "\n",
    "        Accuracy Scale:\n",
    "        1 (Poor): Contains factual errors or misleading information.\n",
    "        2 (Good): Mostly accurate but has minor errors or omissions.\n",
    "        3 (Excellent): Highly accurate and comprehensive, with no factual errors.\n",
    "\n",
    "        Style Scale:\n",
    "        1 (Poor): Too formal, uses some overly complex words.\n",
    "        2 (Good): Good balance of technical content and accessibility, but still uses formal words and expressions.\n",
    "        3 (Excellent): Perfectly accessible langugage for blog/social media, uses simple but precise technical terms when necessary.\n",
    "\n",
    "        Example of bad style: The 4-2-4 formation is instrumental in cultivating an offensive style of play, achieved by deploying a substantial contingent of players in advanced positions, thereby presenting a formidable challenge to the opposing defense.\n",
    "        Example of excellent style: The 4 2 4 formation facilitates attacking play by positioning significant numbers of players high up the pitch, directly challenging the opposition's defense.\n",
    "\n",
    "        Instruction: {instruction}\n",
    "\n",
    "        Answer: {answer}\n",
    "\n",
    "        Provide your evaluation in JSON format with the following structure:\n",
    "\n",
    "        {{\n",
    "            \"accuracy\": {{\n",
    "                \"analysis\": \"...\",\n",
    "                \"score\": 0\n",
    "            }},\n",
    "            \"style\": {{\n",
    "                \"analysis\": \"...\",\n",
    "                \"score\" : 0\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant who evaluates answers based on accuracy and style. Provide your response in JSON format with a short analysis and score for each criterion.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_object\",\n",
    "            },\n",
    "            max_tokens=1000,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    return json.loads(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baaaadf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(batch, start_index):\n",
    "    client = OpenAI()\n",
    "    return [(i, evaluate_answer(instr, ans, client)) for i, (instr, ans) in enumerate(batch, start=start_index)]\n",
    "\n",
    "\n",
    "def evaluate_answers(model_id: str, num_threads: int = 10, batch_size: int = 5) -> Dataset:\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(f\"tunahankilic/{model_id.split('/')[-1]}-results\", split=\"all\")\n",
    "\n",
    "    # Create batches of instruction-answer pairs with their original indices\n",
    "    batches = [\n",
    "        (i, list(zip(dataset[\"instruction\"][i : i + batch_size], dataset[\"answers\"][i : i + batch_size], strict=False)))\n",
    "        for i in range(0, len(dataset), batch_size)\n",
    "    ]\n",
    "\n",
    "    evaluations = [None] * len(dataset)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(evaluate_batch, batch, start_index) for start_index, batch in batches]\n",
    "\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            for index, evaluation in future.result():\n",
    "                evaluations[index] = evaluation\n",
    "\n",
    "    # Replace the 'evaluation' column if it exists, otherwise add it\n",
    "    if \"evaluation\" in dataset.column_names:\n",
    "        dataset = dataset.remove_columns([\"evaluation\"])\n",
    "    dataset = dataset.add_column(\"evaluation\", evaluations)\n",
    "\n",
    "    # Post-process evaluations\n",
    "    accuracy_scores = []\n",
    "    style_scores = []\n",
    "\n",
    "    for evaluation in dataset[\"evaluation\"]:\n",
    "        try:\n",
    "            eval_dict = json.loads(evaluation) if isinstance(evaluation, str) else evaluation\n",
    "            accuracy_score = eval_dict[\"accuracy\"][\"score\"]\n",
    "            style_score = eval_dict[\"style\"][\"score\"]\n",
    "\n",
    "            accuracy_scores.append(accuracy_score)\n",
    "            style_scores.append(style_score)\n",
    "\n",
    "        except (json.JSONDecodeError, KeyError, TypeError):\n",
    "            # If there's an error, append None to maintain alignment\n",
    "            accuracy_scores.append(None)\n",
    "            style_scores.append(None)\n",
    "\n",
    "    # Add new columns to the dataset\n",
    "    if \"accuracy\" in dataset.column_names:\n",
    "        dataset = dataset.remove_columns([\"accuracy\"])\n",
    "    dataset = dataset.add_column(\"accuracy\", accuracy_scores)\n",
    "    if \"style\" in dataset.column_names:\n",
    "        dataset = dataset.remove_columns([\"style\"])\n",
    "    dataset = dataset.add_column(\"style\", style_scores)\n",
    "\n",
    "    dataset.push_to_hub(f\"tunahankilic/{model_id.split('/')[-1]}-results\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2439670",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    \"tunahankilic/LlamaFootball-3.1-8B\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eda8d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating answers for model: tunahankilic/LlamaFootball-3.1-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba5204673d743a68c24f4e4ab0aec17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c5c0c2f9fe4f68b708bc8927ed3697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d515c49db35549ea8b5c4c9834ca21f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for model: tunahankilic/LlamaFootball-3.1-8B\n",
      "Evaluating answers for model: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9620d92174654e6ca666b63ee3de9b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/488 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5704a9de1d2241aaa3a9db8ebb76317a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/62.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5b89fc1e16459f98e7d0c7dfd2988f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cc52f016f540958f68a75932db4127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5529cf6d7e164619ac0ac7dc451a1f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928bb26940e74de7974caa67d2b61055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for model: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "for model_id in model_ids:\n",
    "    print(f\"Evaluating answers for model: {model_id}\")\n",
    "    evaluate_answers(model_id)\n",
    "    print(f\"Evaluation completed for model: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7373bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541dbab565b64c2db14f57d59b73cdcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18c8a1325b64e60b53a648fbf81ca15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/64.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773026dc8dc04e609dc91a1899442763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaFootball-3.1-8B - Accuracy: 2.81\n",
      "LlamaFootball-3.1-8B - Style: 2.52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c747804d29494e7f9dc4a4eaab221976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ead6e202e74bd98f74fb80ae13ab54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/84.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa7f27114e64e9ab1242ca3f571e25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.1-8B-Instruct - Accuracy: 2.64\n",
      "Llama-3.1-8B-Instruct - Style: 2.21\n"
     ]
    }
   ],
   "source": [
    "    # Analyze results\n",
    "    for model_id in model_ids:\n",
    "        dataset = load_dataset(f\"tunahankilic/{model_id.split('/')[-1]}-results\", split=\"all\")\n",
    "\n",
    "        score = sum(dataset[\"accuracy\"]) / len(dataset[\"accuracy\"])\n",
    "        print(f\"{model_id.split('/')[-1]} - Accuracy: {score:.2f}\")  # noqa\n",
    "\n",
    "        score = sum(dataset[\"style\"]) / len(dataset[\"style\"])\n",
    "        print(f\"{model_id.split('/')[-1]} - Style: {score:.2f}\")  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ec399c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_engineering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdomain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqueries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Query\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_engineering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplication\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrag\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquery_expansion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryExpansion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Developer/LLM-Twin/llm_engineering/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_engineering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m application, domain, infrastructure\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_engineering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33msettings\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapplication\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minfrastructure\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Developer/LLM-Twin/llm_engineering/application/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m      3\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mutils\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Developer/LLM-Twin/llm_engineering/application/utils/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m misc\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#from .split_user_full_name import split_user_full_name\u001b[39;00m\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mmisc\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Developer/LLM-Twin/llm_engineering/application/utils/misc.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Generator\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_engineering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten\u001b[39m(nested_list: \u001b[38;5;28mlist\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from llm_engineering.domain.queries import Query\n",
    "from llm_engineering.application.rag.query_expansion import QueryExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af9c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mHTTP Request: POST \u001b[0m\u001b[34mhttps://api.openai.com/v1/chat/completions\u001b[37m \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "Write an article about the effective areas of the 3 5 2 formation.\n",
      "What are the key strengths and effective zones of the 3-5-2 formation in football?\n",
      "Can you explain how the 3-5-2 formation optimizes player positioning and tactical advantages on the field?\n"
     ]
    }
   ],
   "source": [
    "query = Query.from_str(\"Write an article about the effective areas of the 3 5 2 formation.\")\n",
    "query_expander = QueryExpansion()\n",
    "expanded_queries = query_expander.generate(query, expand_to_n=3)\n",
    "for expanded_query in expanded_queries:\n",
    "    print(expanded_query.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-Twin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
